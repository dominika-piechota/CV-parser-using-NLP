{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60ff80b",
   "metadata": {},
   "source": [
    "# ATS Insight: Intelligent Resume Analyzer\n",
    "## Data Management Final Project | University of Liège\n",
    "\n",
    "**Author:** Dominika Piechota\n",
    "**Course:** Data Management\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction & Motivation\n",
    "In the modern recruitment landscape, companies rely heavily on **ATS (Applicant Tracking Systems)** to automatically filter millions of resumes. This creates a \"Black Box\" phenomenon: candidates often submit well-qualified CVs but receive no feedback because their documents fail technical parsing or keyword matching criteria.\n",
    "\n",
    "**The Goal:**\n",
    "This project aims to reverse-engineer this process. We built an end-to-end pipeline that processes raw PDF resumes (including scanned images) and uses Machine Learning to predict a **\"Fit Score\" (1-5)** against specific IT job roles.\n",
    "\n",
    "**Research Questions:**\n",
    "1.  **Extraction:** How can unstructured data be extracted from diverse PDF formats (digital vs. scans)?\n",
    "2.  **Scoring:** Can we train a model to objectively score a CV?\n",
    "3.  **Impact:** Which features (Skills vs. Education) drive the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297ab7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/domin/OneDrive/Dokumenty/GitHub/CV-parser-using-NLP/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "sys.path.append(str(current_dir))\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = current_dir / \"database\" / \"resumes_dataset_CLEANED.jsonl\"\n",
    "IMG_PATH = current_dir / \"analysis_results\"\n",
    "\n",
    "print(\"Setup complete. Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b95f76",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Data Management: The Hybrid Extraction Pipeline\n",
    "\n",
    "The biggest challenge in this project was handling the variety of PDF formats. Standard Python libraries often fail when reading \"scanned\" PDFs (images).\n",
    "\n",
    "**My Solution: Two-Layer Extraction**\n",
    "I implemented a robust extraction engine (in `dev_tools/read_cv.py`) that uses a fallback mechanism:\n",
    "1.  **Layer 1 (Digital):** Attempts to read text using `PyMuPDF`. Fast and accurate for native PDFs.\n",
    "2.  **Layer 2 (OCR):** If the text is empty or too short (<50 chars), the system detects a \"Scan\" and triggers `Tesseract OCR` (via `pdf2image`) to reconstruct the text from pixels.\n",
    "\n",
    "Below is a sample of the processed dataset (`.jsonl` format) after the ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7843e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the cleaned dataset to demonstrate data structure\n",
    "try:\n",
    "    df = pd.read_json(DATA_PATH, lines=True)\n",
    "    print(f\"Dataset successfully loaded.\")\n",
    "    print(f\"Total records (after augmentation): {len(df)}\")\n",
    "    print(\"\\nSample Data (First 3 records):\")\n",
    "    display(df[['target_role', 'score', 'features']].head(3))\n",
    "except ValueError:\n",
    "    print(\"Dataset file not found. Please run 'dev_tools/read_cv.py' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb544eb",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Descriptive Analysis\n",
    "\n",
    "Before modeling, I performed a deep descriptive analysis to understand the biases and characteristics of the dataset. The following visualizations were generated using `matplotlib` and `seaborn`.\n",
    "\n",
    "#### 3.1. Class Distribution\n",
    "I analyzed the number of resumes per role to ensure the dataset is balanced enough for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88926d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=IMG_PATH / \"1_class_distribution.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566dbbf",
   "metadata": {},
   "source": [
    "#### 3.2. Education Level Analysis (Heatmap)\n",
    "This heatmap answers Research Question #3 regarding the impact of education.\n",
    "* **Insight:** We observe a strong correlation between **Data Science** roles and higher degrees (**Master/PhD**).\n",
    "* **Contrast:** Web Development roles (React/Web Developer) show a higher tolerance for **Bachelor** degrees or non-formal education, prioritizing skills over diplomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceae09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=IMG_PATH / \"3_education_percent.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a084d7",
   "metadata": {},
   "source": [
    "#### 3.3. Semantic Analysis (Top Keywords)\n",
    "To validate that our NLP extraction works correctly, we extracted the top 3 most frequent technical skills for each role.\n",
    "* **Validation:** The results are semantically correct (e.g., *Java* for Java Developer, *Python* for Data Scientist), which confirms the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41245dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=IMG_PATH / \"4_top3_skills_per_role.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7cd32d",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Methodology: NLP & Machine Learning\n",
    "\n",
    "To predict the \"Fit Score\", I designed a supervised regression pipeline.\n",
    "\n",
    "#### 4.1. NLP Feature Engineering\n",
    "Raw text cannot be fed into a model directly. I used `nlp.py` to transform text into numerical vectors:\n",
    "1.  **Cleaning:** Removal of stopwords, emails, and phone numbers.\n",
    "2.  **TF-IDF Vectorization:**\n",
    "    * **Summary:** Max 500 features (captures context like \"management\", \"lead\").\n",
    "    * **Skills:** Max 300 features (captures hard skills like \"SQL\", \"Docker\").\n",
    "3.  **Ordinal Encoding:** Education mapped to a rank (High School=1 ... PhD=5).\n",
    "\n",
    "#### 4.2. Model Architecture\n",
    "I chose **`HistGradientBoostingRegressor`** (Histogram-based Gradient Boosting).\n",
    "* **Why?** It natively handles missing values (NaNs) and is highly efficient for datasets with >10k rows.\n",
    "* **Training Strategy:** I used **Undersampling** to balance the dataset (reducing the number of low-score samples) to prevent the model from bias towards the majority class.\n",
    "\n",
    "**Model Evaluation Metrics (Test Set):**\n",
    "* **R2 Score:** Positive (Indicates the model learns patterns better than random guessing).\n",
    "* **MAE (Mean Absolute Error):** ~1.0. This means the model's prediction is typically within 1 point of the \"True\" score. While not perfect, it acts as an effective **suitability filter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d921bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model to show its structure\n",
    "model_path = current_dir / \"models\" / \"best_model_balanced.pkl\"\n",
    "\n",
    "if model_path.exists():\n",
    "    model = joblib.load(model_path)\n",
    "    print(\"✅ Model loaded successfully.\")\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "    print(\"\\nHyperparameters:\")\n",
    "    print(model.named_steps['reg'].get_params())\n",
    "else:\n",
    "    print(\"Model not found. Please run 'dev_tools/chose_model.py' to train it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bfe20",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Final Conclusion\n",
    "\n",
    "This project successfully demonstrated that it is possible to automate the initial screening of candidates using NLP and Machine Learning.\n",
    "\n",
    "**Key Achievements:**\n",
    "1.  **Hybrid Parsing:** Created a system resilient to scanned PDFs.\n",
    "2.  **Interpretability:** Visual analysis confirmed that the model learns intuitive rules (e.g., Python is crucial for Data Science).\n",
    "3.  **End-to-End Pipeline:** The system works from raw PDF input to final score prediction.\n",
    "\n",
    "**Future Improvements:**\n",
    "The primary limitation is the use of synthetic scoring logic for training. Acquiring a real-world dataset labeled by HR professionals would significantly lower the Mean Absolute Error and improve precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
